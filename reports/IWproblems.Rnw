% !TEX encoding = UTF-8 Unicode
\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{color}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{amsfonts}
\usepackage{authblk}
\usepackage{url}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=black, citecolor=black, filecolor=black, urlcolor=black}

% margins
\usepackage[top=1.5in, bottom=1in, left=1in, right=1in]{geometry}
\parskip 3mm

%\graphicspath{{figs/}}

% for comments
\newcommand{\jarad}[1]{\textcolor{Orange}{(jarad: #1)}}
\newcommand{\nacho}[1]{\textcolor{blue}{(nacho: #1)}}
\newcommand{\matt}[1]{\textcolor{red}{(matt: #1)}}

% ....
\newcommand{\I}{\mathrm{I}}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.7}

% for cites
\bibpunct{(}{)}{;}{a}{}{,} 

%-------------------------------------

\title{Perils of the Inverse Wishart}
% \author[1]{Ignacio Alvarez }
% \author[1]{Jarad Niemi }
% \author[2]{ Matt Simpson}
% \affil[1]{Department of Statistics, Iowa State University}
% \affil[2]{Department of Statistics and Department of Economics, Iowa State University}

\date{Unknown (?) }

\begin{document}
 
\maketitle 
 
\begin{abstract}
Covariance matrix estimation arises in multivariate problems including multivariate normal sampling models and regression models where random effects are jointly modeled, e.g. random-intercept, random-slope models. A Bayesian analysis of these problems requires a prior on the covariance matrix, the inverse Wishart prior is conditionally conjugate prior in the normal model and the most frequently used in practice. It is knonw this piror have important problems, however the impact of these issues on posterior inference remians somewhat not understood. Here we evaluate posterior inference of a covariance matrix when inverse wishart prior is used, in the context of several statistical models. We do this through a simulation study and application to a real data set. The main issue on the posterior inference appears when the true variance is small relative to prior mean. In this case, the posterior for the variance is biased toward larger values and the correlation is biased toward zero. This bias persists even for large sample sizes and therefore caution should be used when using the inverse Wishart prior.
\end{abstract}



\tableofcontents
\newpage


\section{Introduction}

\begin{itemize}
  \item $\Sigma$ priors comonlly used
  \item $IW$ problems: lack of flexibility becasuse 1 df parameter, prior dependency among correlation and variances. Also the low density region for inverse-chi2 mention in Gelman. 
  \item Focus: impact on posterior inferences of these problems, specially the last one. 
  \item $IW$ ''incidence'': R packages or articles that use this prior. 
\end{itemize}

The natural conjugate prior for normally distributed data is the Inverse-Wishart distribution. There are two main problems that been pointed out with this prior. First the uncertainty of all the variances is controlled by one single degrees-of-freedom parameter, which may results in lacks of flexibility for the inference (see Sec. 19.2 of \cite{bda2003}). Secondlly, it impose a prior dependence amnog variances and correlations \citep{visualize}.    

\cite{gelman2006prior} shows scaled inv-$\chi^2$ distribution has extremely low density in a region near zero and thus causes bias in the result posteriors for these variances. This issue is extended to the inverse wishart case since the implied distribution on each individual variance is actually inv-$\chi^2$.    

These issues are focus on characteristics of the prior. However, the impact of these characteristics on the posterior distribution and the inference are not studied that much. We show how the inference about individual vairances and correlations is afected when we use $IW$ as a prior. The main impact is due to the low density region of the inv-$\chi^2$ density which may explain bias in the posterior inference for $\Sigma$ when the true variances are low. 

Although all the problems that $IW$ prior has, is still widelly used in aplications. Several R packages has built in functions that uses $IW$ as a prior where the user only may be able to change the hyperparameter values\footnote{packages: {\tt nicheROVER,SharedHT2 (not in cran anymore), Boom, sbgcop, MCMCglmm,BayesComm, MSBVAR,bayesSurv,phcfM, monomvn, miscF, bayesm, spBayes, factorQR, agRee, DPpackage } } (and sometimes not even this). In this sense it would be nice to find some solution to its problems as a prior without designing a new one. 
%Many multivariate porblems invole the estimation of a covariance matrix. In Bayesian framework, several differents priors has been proposed to make inference about a covariance matrix. Some alternatives are based in decomposing the covariance matrix in smaller components and set priors for each components, for instance \cite{yang1994} and \cite{barnard2000} uses this type of strategy. A different aproach is to build hirerarchiccal model for the covariance matrix entries and yet a third way is to set prior for some transformation the covariance matrix. \cite{daniels1999} and \cite{matilde} are examples of the hirerarchical aproach and \cite{leonard1992} set prior on the presition matrix. Perhaps one of the reasons for having many alternatives ways to set a prior for a covariance matrix is that the usual default options for setting a prior present important drawbacks. The non-informative Jeffrey prior may result on improper posterior when it is used within the context of hirerarchical linear model. 

\section{Inverse Wishart}

A simetric, positive defnite random $d\times d$ matrix $\Sigma$ with elements $\sigma_{ij}$ has an Inverse Wishart distribution ($IW$) when its distribution is 
\begin{equation}
p(\Sigma) \propto |\Sigma|^{-(\frac{\nu +d+1}{2})} e^{-\frac{1}{2}tr(\Lambda\Sigma^{-1})}
\label{IW}
\end{equation}

where $\Lambda$ is a positive definite $d$ dimensional matrix and $\nu$ is a scalar degrees of freedom parameter satisfying $\nu>d-1$, we denote this distribution by $\Sigma \sim IW(\nu, \Lambda)$. Moments :
\[\begin{array}{lcl}
E(\Sigma) & = &  \Sigma_0= \frac{\Lambda}{\nu - d - 1}$ for $\nu>d+1. \\
Cov(\sigma_{ij}, \sigma_{kl}) &=& ... \\
E(\Sigma A \Sigma) &=& ... \\
E(tr(A\Sigma)\Sigma) &=& ... \\
E(tr(A\Sigma)tr(B\Sigma) &=& ... \\
\end{array}
\]


\subsection{Inverse Gamma}
The special case $d=1$ \eqref{IW} is reduced to $\sigma^{-(\frac{\nu+2}{2})} e^{-\frac{\lambda}{2\sigma^2}}$ which means $\sigma^2 \sim \mbox{inv-}\chi^2(\nu/2, \lambda/\nu)$ an scaled inverse chi-square \footnote{The scaled inverse chi-square denoted by $X \sim \mbox{inv-}\chi^2(\nu, s^2)$ has a density function given by $p(x) =  \frac{(\nu/2)^{\nu/2}} {\Gamma(\nu/2)} s^{\nu}x^{-(\nu/2 + 1)} \mbox{exp}\left\{-\nu s^2 / 2x\right\} $} or inverse gamma distribution. 

\subsection{Marginals}
The matrix $\Sigma$ will be considered as a covariance matrix for some random vector. We refer to the standard deviations $\sigma_i$ and correlations $\rho_{ij}$ where $\sigma_i^2=\sigma_{ii}$ and $\sigma_{ij} = \rho_{ij}\sigma_i\sigma_j$. 

Lets consider the matrices $\Sigma_{11}\; \Sigma_{12}\; \Sigma_{22}$ with dimensions $d_1\times d_1$, $d_1\times d_2$,$d_2\times d_2$ respectively forming a  partition of the $\Sigma$ matrix 

\[ \Sigma = 
\begin{bmatrix}
\Sigma_{11} & \Sigma_{12} \\ 
\Sigma_{21} & \Sigma_{22}
\end{bmatrix}
\]
and let further define $\Sigma_{22.1} = \Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12}$ and $ T = \Sigma_{11}^{-1}\Sigma_{12}$, then \cite{box1973bayesian} show that: 

\begin{equation}
\begin{array}{ll}
\Sigma_{11} \sim &  IW(\nu, \Lambda_{11}) \\
\Sigma_{22.1} \sim &  IW(\nu+d_1, \Lambda_{22.1}) \\
T \sim &  t_{d_1 d_2}(\hat T,\Lambda_{11}^{-1},\nu+d_1,\Lambda_{22.1}) ????? 
\end{array}
\label{IWmargin}
\end{equation}

Ditribution for individual vairances is directlly obtained from \eqref{IWmargin}. Seting $d_1=1$ we obtain the distribution for $\sigma_1^2$ and this can be extendend to any variance, specifically for each variance $\sigma_i^2\sim \mbox{inv-}\chi^2(\nu - d + 1, \frac{\lambda_{ii}}{\nu-d+1} )$ where $\lambda_{ii}$ is the $i^{th}$ diagonal entry of $\Lambda$.

Asuming $\Lambda$ matrix is diagonal with elements $\lambda_i$ it is posible obtain the distribution for the corresponding correlation matrix. Let $\Sigma = S R S$, where $S$ is a diagonal matrix with standard deviations, $s_i=\sqrt{\sigma_{ii}}$ and R a matrix with the correlations, then we can derive 
\[f(R) \propto |R|^{-(\frac{\nu +d+1}{2})} \left(\prod_i r^{ii}\lambda_i \right)^{\nu/2} \]

and further assuming $\Lambda=I$ we can show each correlation is distributed as Beta on $(-1,1)$ \citep{barnard2000}.

With a general definite positive matrix $\Lambda$  we can use that any submatrix is also distributed as $IW$, equation \eqref{IWmargin}. This fact can be used to obtain the posterior distribution of the correlation coeficient. In particular, taking the first submatrix of dimension $d=2$ we have 
\[
p(\sigma_1^2,\sigma_2^2,\sigma_{12}) \propto (\sigma_1^2\sigma_2^2 -\sigma_{12}^2)^{-\frac{\nu+d+1}2}exp\left [-\frac{1}{2(\sigma_1^2\sigma_2^2 -\sigma_{12}^2)}   (\lambda_{11}\sigma_2^2 - 2\lambda_{12}\sigma_{12} + \lambda\sigma_1^2 \right] 
\]
Then, following \cite{box1973bayesian}\footnote{Actually they used a transformation proposed by Fisher to obtain the sampling distribution of the pearson correlation coeficient.} we consider $r_0=\lambda_{12}/\sqrt{\lambda_{11}\lambda{22}}$, and the transformation 
\[
\begin{array}{llll}
x = (\frac{\sigma_i\sigma_j}{\lambda_{ii}\lambda_{jj}})^{1/2} & 
w = (\frac{\sigma_i \lambda_{jj}}{\lambda_{ii}\sigma_j})^{1/2} & 
\rho_{ij} = \frac{\sigma_{ij}}{\sigma_i\sigma_j}  & J = \frac{2x^2(\lambda_{ii}\lambda_{jj})^{3/2}}{w}
\end{array}
\]

% \[
% \begin{array}{ll}
% p(x,w,\rho) & \propto (x^2(1-\rho^2))^{-\frac{n+2}{2}}\frac{x^2}{w}exp\left [-\frac{1}{2x(1-\rho^2)} (w^{-1} - 2\tilde r\rho w) \right] \\
% p(w,\rho) & \propto \frac{(1-\rho^2)^{-\frac{n+2}{2}}}{w} \int x^{-n}exp\left [-\frac{1}{2x(1-\rho^2)} (w^{-1} - 2\tilde r\rho w) \right] dx \\
% p(w,\rho) & \propto \frac{(1-\rho^2)^{-\frac{n+2}{2}}}{w} (1-\rho^2)^{n-1} (w^{-1} - 2\tilde r\rho w)^{-(n-1)} \\
% \end{array}
% \]

to obtain the marginal distribution of the correlation coeficient
\begin{equation}
p(\rho_{ij})  \propto (1-\rho_{ij}^2)^{\frac{\nu+d+1}{2} -3} \int_0^\infty w^{-1} (w^{-1} - 2r_0 \rho_{ij} + w)^{-(\nu+d-2)}dw
\label{rho.iw}
\end{equation}

\subsection{Wishart}
The inverse of an $IW$ distributed matrix has a Wishart distribution. When $\Sigma\sim IW(\nu,\Lambda)$ then $\Sigma^{-1}\sim W(\nu-d-1,\Lambda^{-1})$

\subsection{Barlet decomposition (choleski?)}

If $\Sigma\sim IW(\nu,I)$ and $\Sigma^{-1} = BB^{\top}$ with $B=(b_{ij}$ lower tirangular matrix then $t_{ij} \stackrel{ind} \sim \chi^2_{d-i+1}$ independent. 

\section{Applications to Bayesian analysis}

\subsection{Models where covariance matrices are estimated}

\subsubsection{Multivariate normal model}
Consider the multivariate normal model, that is let $Y_i\in \mathbb{R}^d$ for $i=1,\ldots,n$ and assume $Y_i \stackrel{iid}{\sim} N(\mu, \Sigma)$ with $\mu\in \mathbb{R}^d$ and $\Sigma$ is a $d$-dimensional positive definite matrix.   
  \begin{equation}
 p(y\vert \mu,\Sigma) \propto |\Sigma|^{-n/2} \mbox{exp}\left\{- \frac{1}{2} \sum_{i=1}^n (y_i-\mu)^\top \Sigma^{-1} (y_i-\mu) \right\} = |\Sigma|^{-n/2}  \mbox{exp}\left\{- \frac{1}{2}  \mbox{tr}(\Sigma^{-1}S_\mu)  \right\} 
 \label{like}
 \end{equation}
The likelihood is provided in equation \eqref{like} where $y$ represents the entire data and  $S_\mu = \sum_{i=1}^n (y_i-\mu) (y_i-\mu) ^\top$. Which can be decompose as $S_{\mu} = A + M =\sum_{i=1}^n(y_i-\bar y)(y_i-\bar y) ^{'} + n(\bar y-\mu)(\bar y-\mu) ^\top$.  

\subsubsection{Hierarchical random effects model}

\subsubsection{Hierarchical regression model}

%\subsubsection{Discriminant analysis}

\subsection{Priors for covariance matrices}

We start by studing setting the inverse withart distribution as prior for $\Sigma$, this will be conjugate in the multivariate normal model. 
To complete model we set uniform prior for $\mu$, $p(\mu)\propto 1$. Posterior distribution for $\Sigma$ is derived as follows, 
\begin{equation}
\begin{array}{ll}
 p(\Sigma\vert y) & \propto  \int p(\Sigma, \mu \vert y) d\mu  \\
                  & \propto p(\Sigma) \int |\Sigma|^{-n/2}e^{-\frac{1}{2}tr(\Sigma^{-1}(A+M))}d\mu  \\
                  & \propto p(\Sigma) |\Sigma|^{-n/2}e^{-\frac{1}{2}tr(\Sigma^{-1}A)} \int e^{-\frac{1}{2}tr(\Sigma^{-1}M)} d\mu  \\
                  & \propto p(\Sigma) |\Sigma|^{(-n/2 + 1/2)}e^{-\frac{1}{2}tr(\Sigma^{-1}A)} \\
                  & \propto |\Sigma|^{-(\frac{\nu+d+n}{2})} e^{-\frac{1}{2}tr(\Sigma^{-1}(A+\Lambda)}
 \end{array}
 \label{muint}
 \end{equation} 
implying that $\Sigma\vert y \sim IW(n+\nu-1, A+\Lambda)$ as expected. 

Now appling $IW$ properties we mentioned previuoslly we can obtain posterior distributions for marginal variances and correlations. Individual vairances are distributed as scaled inverse chi-square distribution $\sigma_i^2\sim \mbox{inv-}\chi^2(\nu - d + 1, \frac{\lambda_{ii}}{\nu-d+1} )$ where $\lambda_{ii}$ is the $i^{th}$ diagonal entry of $\Lambda$. For the correlation, once again we consider the first submatrix of dimension $d=2$ and letting $\tilde A = A + \Lambda$  we have 
% \[
% p(\sigma_1^2,\sigma_2^2,\sigma_{12}) \propto (\sigma_1^2\sigma_2^2 -\sigma_{12}^2)^{-\frac{n+2}{2}} exp\left [-\frac{1}{2(\sigma_1^2\sigma_2^2 -\sigma_{12}^2)}   (\tilde a_{11}\sigma_2^2 - 2\tilde a_{12}\sigma_{12} + \tilde a_{22}\sigma_1^2 \right] 
% \] 
% where $a_{kj}=\sum_{i=1}^n(y_{ik}-\bar y_k)(y_{ij}-\bar y_j)$ and $\tilde a_{ij} = a_{ij}+\lambda_{ij}$. 
% Then we follow the derivations in \cite{box1973bayesian} to obtain the posterior density of the correlation when a Jefrey prior is consider. They used a transformation proposed by Fisher to obtain the sampling distribution of the pearson correlation coeficient. Let $r=a_{12}/\sqrt{a_{11}a{22}}$ and $r=\tilde a_{12}/\sqrt{\tilde  a_{11}\tilde  a{22}}$
% \[
% \begin{array}{llll}
% x = (\frac{\sigma_1\sigma_2}{\tilde a_{11}\tilde a_{22}})^{1/2} & w = (\frac{\sigma_1 \tilde a_{22}}{\tilde a_{11}\sigma_2})^{1/2} & \rho = \frac{\sigma_{12}}{\sigma_1\sigma_2}  & J = \frac{2x^2(\tilde a_{11}\tilde a_{22})^{3/2}}{w}
% \end{array}
% \]
% \[
% \begin{array}{ll}
% p(x,w,\rho) & \propto (x^2(1-\rho^2))^{-\frac{n+2}{2}}\frac{x^2}{w}exp\left [-\frac{1}{2x(1-\rho^2)} (w^{-1} - 2\tilde r\rho w) \right] \\
% p(w,\rho) & \propto \frac{(1-\rho^2)^{-\frac{n+2}{2}}}{w} \int x^{-n}exp\left [-\frac{1}{2x(1-\rho^2)} (w^{-1} - 2\tilde r\rho w) \right] dx \\
% p(w,\rho) & \propto \frac{(1-\rho^2)^{-\frac{n+2}{2}}}{w} (1-\rho^2)^{n-1} (w^{-1} - 2\tilde r\rho w)^{-(n-1)} \\
% \end{array}
% \]
% Finnally, 
\begin{equation}
p(\rho\vert y)  \propto (1-\rho^2)^{\frac{n+\nu}{2} -2} \int_0^\infty w^{-1} (w^{-1} - 2\tilde r \rho + w)^{-(n+\nu-1)}dw
\label{rhopost.iw}
\end{equation}

wehre $r=\tilde a_{12}/\sqrt{\tilde  a_{11}\tilde  a{22}}$

\subsubsection{Jefrey Prior}

A non informative prior for the covariance matrix is $p(\Sigma)\propto |\Sigma|^{-(\frac{d+1}{2})}$. 

In the multivariate normal model contex, posterior inference about covariance matrix based on Jefrey prior can be easily obtain as  $p(\Sigma\vert y) \propto |\Sigma|^{-(\frac{d+n}{2})} e^{-\frac{1}{2}tr(\Sigma^{-1}A)}$ implying that $\Sigma\vert y \sim IW(n-1, A)$. 

Box-Tiao finds the posterior distribution for $\rho$ using a transformation proposed by Fisher to obtain the sampling distribution of the pearson correlation coeficient, $r=a_{12}/\sqrt{a_{11}a{22}}$
\begin{equation}
p(\rho\vert y)  \propto (1-\rho^2)^{n/2 -2} \int_0^\infty w^{-1} (w^{-1} - 2r\rho + w)^{-(n-1)}dw
\label{rhopost.jef}
\end{equation}

This has nice properties in the multivariate normal model contex. However \cite{berger2005posterior} shows it may result on improper posterior when it is used in hierarchical linear models.


\subsubsection{Scaled inverse Wishart}

An alternative to the IW prior is the scaled inverse Wishart (SIW) prior which is based on the inverse Wishart distribution but adds additional parameters for flexibility \citep{odomain}. The SIW prior defines $\Sigma \equiv \Delta Q \Delta $ where $\Delta$ is a diagonal matrix with $\Delta_{ii}=\delta_i$, then 

\begin{equation}
Q \sim  IW(\nu, \Lambda) \;\;, \;\; \log(\delta_i) \stackrel{ind} \sim N(b_i, \xi_i^2)
\label{eq:siw}
\end{equation} 

We use the notation $\Sigma \sim SIW(\nu, \Lambda, b, \xi)$ to refer to this prior.  By construction, the SIW prior implies that $\sigma_i = \delta_i \sqrt{Q_{ii}}$, and $\Sigma_{ij}=\delta_i\delta_jQ_{ij}$. Thus each standard deviation is the product of a log-normal and the square root of a scaled inv-$\chi^2$ and the correlations $\rho_{ij} = Q_{ij}/\sqrt{Q_{ii}Q_{jj}}$ have the same distribution they had under the inverse Wishart on $Q$.  


\subsubsection{Hierarchical inverse Wishart}
\subsubsection{Separation strategy}
\subsubsection{Others?}


\section{Solutions ...}

\subsection{Multivariate normal model}
\begin{itemize}
  \item uses small values for diagonal elements in the prior, instead of $I$ matrix.
  \item use sample vairance on the pior matrix. 
\end{itemize}

\subsection{Hierarchical models}

\section{Results}

\subsection{Understanding the inference bias}
show why posterior is biased when true variance is low...






\bibliographystyle{asa}      
\bibliography{bayes}      

\end{document}
